# Page 1

Machine learning models for the secondary Bjerknes force between two insonated bubbles  Haiyan Chen, Yue Zeng  School of Material and Energy, Guangdong University of Technology, Guangzhou, China, 510006  Yi Li ∗  School of Mathematics and Statistics, University of She ffi eld, She ffi eld, UK, S3 7RH  Abstract  The secondary Bjerknes force plays a significant role in the evolution of bubble clusters.   However, due to the complex dependence of the force on multiple parameters, it is highly non-trivial to include the e ff ects of this force in the simulations of bubble clusters. In this paper, machine learning is used to develop a data-driven model for the secondary Bjerknes force between two insonated bubbles as a function of the equilibrium radii of the bubbles, the distance between the bubbles, the amplitude and the frequency of the pressure. The force varies over several orders of magnitude, which poses a serious challenge for the usual machine learning models. To overcome this di ffi culty, the magnitudes and the signs of the force are separated and modelled separately. A nonlinear regression is obtained with a feed-forward network model for the   logarithm   of the magnitude, whereas the sign is modelled by a support- vector machine model. The principle, the practical aspects related to the training and validation of the machine models are introduced.   The predictions from the models are checked against the values computed from the Keller-Miksis equations. The results show that the models are extremely e ffi cient while providing accurate estimate of the force. The models make it computationally feasible for the future simulations of the bubble clusters to include the e ff ects of the secondary Bjerknes force.  Keywords:   Bubble clusters, secondary Bjerknes force, machine learning, neural networks, support-vector machine, numerical simulations  1. Introduction  The secondary Bjerknes force [23, 4] is the interac- tion between two bubbles oscillating in a acoustically driven fluid, and it is induced by the pressure pertur- bation radiated from the bubbles. The force is thought to be important in the evolution of bubble clusters and has attracted considerable research in the past decades [8, 31, 32, 9, 26, 2, 16, 30, 35, 19, 36], which explores the e ff ects of nonlinear correction, multiple scattering, and the coupling with shape oscillation and translation, as well as the experimental measurement of the force. The asymmetricity of the force is discussed recently in [28] taking into account higher order nonlinear coupling between the bubbles, which further highlights the com- plexity of the force. Recent experimental evidences [12, 22] support the importance of the secondary Bjerknes force in the dy-  ∗ To whom correspondence should be addressed.  Email address:   yili@sheffield.ac.uk.   (Yi Li)  namics of micro-bubble clusters. The collective behav- iors of up to 100 oscillating bubbles are modelled in [15] using the coupled Keller-Miksis equations [20]. It is found that the interactions between the bubbles can be both constructive and destructive, and the bifurcation sequences of a system with more bubbles can be much di ff erent from a small one. The research again demon- strates the importance of the interactions between the bubbles which are manifested as the secondary Bjerknes force.   The force has been used to manipulate bubbles, e.g., as a mean to control micro-devices, which poten- tially have important applications [18, 21, 1]. Given that bubble clusters are commonly observed in biomedicine, metallurgical industries, food processing, and other ap- plications (see, e.g., [5, 3, 33, 10]), the modelling of the secondary Bjerknes force and hence bubble clusters is a question of significant interests. Few simulations of bubble clusters so far have em- ployed sophisticated models for the secondary Bjerknes force. Numerical simulations conducted in [22], with a simple model for the secondary Bjerknes force, quali-  Preprint submitted to Ultrasonics Sonochemistry   January 24, 2020  arXiv:2001.08291v1 [physics.comp-ph] 20 Jan 2020

# Page 2

tatively reproduce the experimental observations on the clustering of bubble clouds.   Similar simplified models are also used in the simulations in [27, 29, 25], which qualitatively reproduces the formation of the Lichtenberg pattern [23]. These simulations follow the movements of individual bubbles, thus are based on a Lagrangian ap- proach. Recently a hybrid Lagrangian-Eulerian method is proposed in [24] where bubble oscillation is computed, although the secondary Bjerknes force is not explicitly included. The past research has yielded considerable physical insights about the secondary Bjerknes force.   Unfortu- nately, due to the complexity of the problem, the insights have yet to be translated into accurate and computation- ally e ffi cient models. We observe, however, that the com- plexity of the problem makes it an excellent example for which a data-driven approach can be fruitful. Data- driven methods, especially machine learning, have made tremendous progresses in recent years, as are exemplified and popularized by the success of AlphaGo [34].   The methods have been successfully applied to many physi- cal and applied sciences. There is, however, not yet any report of such applications in bubble simulations.   The objective of this paper is to use machine learning to build a novel model for the secondary Bjerknes force that is more comprehensive than those previously reported, and more generally, introduce this useful method into the in- vestigation and modelling of bubbles oscillations. The paper is organized as follows.   The dynamical equations for the bubbles are reviewed in Section 2, where the dependence of the secondary Bjerknes force on relevant parameters are highlighted. The data set for the force is described in Section 3. Section 4 introduces the relevant machine learning models to be used to build the model for the force. The practical aspects of the train- ing and testing of the models are also presented. Addi- tional checks are performed in Section 5 where the e ffi - ciency of the models is also assessed. The conclusions are summarized in Section 6.  2. The governing equations  Let   D   be the distance between the two bubbles. The radius of bubble   i   ( i   =   1 ,   2) is denoted by   R i ( t ) and its equilibrium radius is   R Ei .   The bubbles are driven by a uniform pressure oscillating harmonically in time:  p I   ( t )   =   p 0   −   p a   sin( ω t )   (1) where   p 0   is the ambient pressure,   p a   is the amplitude of the ultrasonic pressure, and   ω   ≡   2 π   f   and   f   are the angular and linear frequencies, respectively. By using a pressure uniform in space, it has been assumed that   D  is small compared with the wave length of the pressure wave or the bubbles are on the same phase plane of a planar pressure wave. The fluid has density   ρ , speed of sound   c , surface tension   σ   and kinematic viscosity   ν . The radii of the bubbles can be described by the Keller-Miksis model [20, 4] with additional pressure coupling terms between the bubbles as introduced in [26]. Ignoring the time-delay e ff ect, the coupling pres- sure between bubbles   i   ( i   =   1 ,   2) and   j   ≡   3   −   i , denoted as   p i j , is given by [26]  p i j ( t )   =   ρ  D dR 2  j   ˙ R   j  dt   ,   (2) which is valid when the radii   R i   and   R   j   are much smaller than   D . With   p i j   included, the equation for   R i ( t ) becomes [26]: 2 ρ (1   −   c − 1   ˙ R i ) R i   ¨ R i   +   ρ (3   −   c − 1   ˙ R i ) ˙ R 2  i  = 2(1   +   c − 1   ˙ R i )( p wi   −   p I   )   +   2 c − 1 R i ( ˙ p wi   −   ˙ p I   )  −   2 ρ D − 1 (2 R   j   ˙ R 2  j   +   R 2  j   ¨ R   j ) ,   (3) where  p wi   =  (  p 0   +   2 σ  R Ei  ) (   R Ei  R i  ) 3 k  −   2 σ  R i  −   4 ρν   ˙ R i  R i  ,   (4) is the pressure on the outer wall of bubble   i   and   k   is the polytropic exponent for the gas inside the bubble.   We note that other models for the oscillation of coupled bub- bles exist in the literature. Obviously the machine learn- ing models to be presented below can be used with other models as well. The secondary Bjerknes force is defined as the time- averaged pressure exerting on bubble   i   due to the oscilla- tions of bubble   j   [8, 26]. Let   F i j   be the notation for this force, simple calculation shows that, for small bubbles,  F i j   can be written as (see, e.g., [8]):  F i j   =   −   ρ  D 2  〈  V i  dR 2  j   ˙ R   j  dt  〉  =   ρ 〈   ˙ V i   ˙ V   j 〉  4 π D 2   ,   (5) where   V i   is the volume of bubble   i . The pointed brack- ets represent time averaging. In the above expression we follow the tradition where   F i j   is positive when it is at- tractive. The secondary Bjerknes force factor   f i j   [26] is defined as  f i j   ≡   D 2   F i j   =   ρ   〈   ˙ V i   ˙ V   j 〉  4 π   .   (6) In a bubble cluster,   F i j   is expected to depend not only on bubbles   i   and   j   but also the other bubbles. Nevertheless, when the force was considered in the few bubble cluster simulations [27, 29, 22] reported so far,   F i j   had all been calculated from 2-bubble systems, where the contribu- tions from other bubbles were neglected. Empirical fit- ting of   F i j   as a function of   D   was used. The dependence of   F i j   on other parameters have not been considered. 2

# Page 3

For a 2-bubble system, the only secondary Bjerknes force factor is   f 12 .   f 12   depends on many parameters of the system, including   R Ei ,   D ,   p a ,   ω ,   ν ,   ρ ,   c ,   σ , and   k .   In the present investigation, we choose water as the medium, hence fixing   ν   at 0 . 89   ×   10 − 6 m 2 / s ,   ρ  at 997kg / m 3 ,   c   at 1497m / s   and   σ   at 0 . 0721 N / m.   An adiabatic process is assumed so that   k   is fixed at 1 . 4, whereas   p 0   is assumed to be the atmospheric pressure  p atm   =   1 . 013   ×   10 5 Pa. The objective of the investigation is to model the dependence of   f 12   (hence   F 12 ) on the five parameters:   D ,   p a ,   ω   (or   f   ),   R E 1   and   R E 2 .  3. The data for   f 12  The machine learning method is used to discover the complicated dependence of   f 12   on the system parame- ters. The method is data-driven and is based on a large data set for   f 12   obtained over a range of values for the five parameters. The distance   D   ranges from 100 μ m to 1000 μ m with an increment of 100 μ m. The pressure am- plitude   p a   ranges from 40kPa to 150kPa with an incre- ment of 10kPa.   This range covers both near harmonic and strongly nonlinear aharmonic oscillations. The forc- ing frequency   f   ranges from 20kHz to 40kHz with an increment of 10kHz. Both   R E 1   and   R E 2   start at 1 μ m and end at 10 μ m with an increment of 2 μ m. When the other parameters are held fixed, a bubble pair with radii ( R E 1 ,   R E 2 ) would have the same   f 12   as a pair with radii ( R E 2 ,   R E 1 ). Therefore the parameter com- binations with   R E 1   <   R E 2   are removed from the data set, which leaves in total 5400 combinations of parameter values. Eq. 3 is numerically integrated for each combi- nation to obtain the corresponding   f 12 . The ode45 solver in MATLAB is used. In each run, the simulation is run for ten periods of the forcing pressure to allow the os- cillation becomes stationary. The data from the last two periods are used to calculate the force factor   f 12   accord- ing to Eq.   6.   The data for   f 12   obtained this way, and the corresponding parameters, form the dataset for the development of the machine learning models. In the ter- minology of machine learning, a set of values for the five parameters is called a   predictor , the corresponding   f 12   is a   response .  4. The machine learning models  The secondary Bjerknes force factor   f 12   depends sen- sitively on the flow parameters. As a result, the magni- tude for   f 12   varies over many orders of magnitude, which poses a significant di ffi culty for the development of the machine learning models. To overcome the di ffi culty, the data set for   f 12   is split into two. The first one contains log 10   |   f 12 | , whereas the second one contains the sign of   f 12   (sgn   f 12 ). Two ma- chine learning models are built for the two sets sepa- rately, and the prediction for   f 12   is reconstructed from the two models. The first model, given the nature of the data, is a regression model, which is implemented with a feed-forward neural network (FFNN). The data in the second data set are binary (they are either 1 or   − 1). A classification model, the support-vector machine (SVM), is thus used. If the predictions from the first and the sec- ond models are   y 1   and   y 2 , respectively, the prediction for  f 12   is then given by   y 2 10 y 1   . Working with log 10   |   f 12 |   proves to be crucial. Taking the logarithm reduces the range of the data, and as a re- sult, a FFNN can be found to model   |   f 12 |   (after expo- nentiation) and hence   f 12   with good accuracy. Without separating the magnitude and the sign and taking the log- arithm of the magnitude, we failed to find a satisfactory ML model for   f 12 . The FFNN and the SVM models are now explained.  4.1. The feed-forward neural network for   log 10   |   f 12 |  An FFNN [14] typically includes an input layer, an out put layer, and several hidden layers of neurons. The neurons are connected to and receive inputs from those in previous layers, and similarly, connected to and send outputs to those in later layers. Each neuron is defined by an activation function (also known as transfer func- tion), which processes the inputs and generates an out- put.   The inputs are combined with suitable weights   w  and biases   b   in the activation function. Fig. 1 provides a schematic illustration of the structure of an FFNN. All the components in a blue box forms a neuron. Only one neuron is shown explicitly in each layer in Fig. 1 but in reality there could be many.   In an FFNN, the number of hidden layers, the number of neurons in each layer, and the activation functions are chosen   a priori   and, in practice, mostly empirically. The weights   w   and biases   b  are determined by ‘training’ the network to provide the optimal description of the data in the so-called ‘train- ing’ dataset. The optimal weights and biases are usually obtained by applying optimization algorithms which ad- just the weights and biases iteratively through a process called back-propagation. For more details on FFNNs and machine learning in general, see, e.g., [14, 17, 13].  4.1.1. The architecture and the hyperparameters  MATLAB is used to define, train, validate and test the network [6]. The numbers of layers and neurons and the activation functions are the hyperparameters that should be decided at the outset. For the activation functions,   the default setting is adopted, where the hyperbolic tangent sigmoid function is used for the neurons in the hidden layers and the linear function is used in the output layer. Even though in the 3

# Page 4

Input 5 Hidden2   Output Output 15 1  Hidden1 15   1 Figure 1:   The architecture of the FFNN with two hidden layers having 15 neurons on each.  machine learning community rectified linear units (Re- LUs) are now recommended for large scale problems, a few tests using the ReLUs for the hidden layers do not show appreciable di ff erences. As for the numbers of the layers and neurons, it is known that perfect regression can be obtained for any dataset if there is no limit to the number of available neurons. However, the training may become too expen- sive and the model too ine ffi cient if too many neurons are used. Therefore it is desirable to use as few neurons as possible. Empirical evidences show that, in some cases, the model performance can be improved by using more hidden layers [13].   However, there is not yet theoreti- cal justification or guidelines for the optimal choices. As a result, we have adopted the following trial-and-error strategy to decide these two hyperparameters, which is explained briefly here while the numerical evidences is presented later.   We start with an FFNN with only one hidden layer, and train it with increasing number of neu- rons, until satisfactory performance is obtained. After a number of tests, it is found that consistently good results can be obtained with 30 neurons.   The total number of neurons is thus fixed at 30, and networks with di ff erent numbers of hidden layers are tested to explore how the performance can be further improved. As demonstrated below with numerical results, the best performance is found with two hidden layers and 15 neurons on each. This architecture is thus chosen, which is illustrated in Fig. 1. More details are given in Section 4.1.3.  4.1.2. The training of the FFNN  With the architecture chosen, the weights and the bi- ases in the neurons are then initialized randomly, but they have to be adjusted to improve the performance of the model. This process is called training, in which the dataset for log 10   |   f 12 |   mentioned in Section 3 is used. For each predictor (i.e., a parameter combination), the FFNN is used to make a prediction for log 10   |   f 12 | , which is the response in this model.   The prediction is com- pared with the true value obtained as explained in Section 3. The mean squared error (MSE) between the true and predicted values is used as the performance measure for the model. The Levenberg-Marquardt algorithm [14] is Architecture   N n   in each layer 1   30 2   20, 10 3   10, 20 4   15, 15 5   5,   10, 15 6   10, 10, 10 7   10, 10, 5,   5 8   10, 5,   5,   5,   5  Table 1: The number of neurons ( N n ) in each layer for each network architecture.  used to optimize the weights and biases iteratively. In the machine learning terminology, an iteration which scans through all training data is called an epoch. The training is stopped when a certain stopping condi- tion is satisfied. One of these conditions is that the mag- nitude of the gradient of the MSE should be su ffi ciently small. However, in our tests, the training is always ter- minated due to detection of overfitting. Overfitting is a phenomenon where the model performs well in training, but makes poor predictions for data outside of the train- ing dataset. It is a common problem to be avoided when training a neural network.   In this investigation, cross- validation is used to tackle this problem [17].   Specifi- cally, 70% of the dataset for log 10   |   f 12 |   is randomly cho- sen to form the training set, while 15% is used for vali- dation, and the rest for testing. In each epoch, the current FFNN is used to make predictions on the validation data set. The MSE of the predictions over the validation set is monitored. If the MSE increases for   N o   =   6 consec- utive epochs (while the MSE for the training set is still decreasing), then overfitting is deemed to have happened and the training is stopped. The value 6 is empirical. If a di ff erent   N o   is used, one might need to adjust other pa- rameters to obtain a model with similar performance.  4.1.3. Numerical results  The architectures of the FFNN models tested in this investigation are summarized in Table 1. For each archi- tecture, the training is run 32 times with random initial- 4

# Page 5

Arch. 1 Arch. 2 Arch. 3 Arch. 4 Arch. 5 Arch. 6 Arch. 7 Arch. 8 0 1 2 3 4 5 Figure 2:   The median and range of the optimal MSE obtained with di ff erent FFNN models in 32 runs. 0   10   20   30   40   50   60   70   80 10 -2  10 -1  10 0  10 1  10 2  Training Validation Testing  0   10   20   30   40   50   60   70   80 10 -1  10 0  10 1  10 2  Figure 3:   The changes of the MSE and the gradient with the epochs for a model with the fourth architecture.  ization. 32 models are thus produced, which are slightly di ff erent from each other even if they have the same ar- chitecture.   The 32 optimal validation MSEs for these models obtained from the training are calculated.   The median values are plotted with the symbols in Fig. 2 for all 8 architectures, as well as the maximum and mini- mum values, which are given by the error bars. The re- sult for architecture 1 shows that, with 30 neurons, the validation MSE can be kept under 5% in all runs. This observation has been the basis to choose 30 as the to- tal number of the neurons.   Fig.   2 shows clearly that the MSE can be reduced by using multiple hidden lay- ers. Architecture 4, which has two hidden layers with 15 neurons in each, displays the best performance. Further increasing the number of hidden layers appears to some- what degrade the performance. Based on these results, architecture 4 has been chosen to obtain all the results to be presented below. As an illustration, Fig. 3 shows how the performance and the gradient of the model improves by the training. -20   -15   -10   -5   0 -20 -15 -10 -5 0  Training Testing  Figure 4:   The regression of the training (filled circles) and the testing (empty circles) data. -0.5 -0.445 -0.39 -0.335 -0.28 -0.225 -0.17 -0.115 -0.06 -0.005 0.05 0.105 0.16 0.215 0.27 0.325 0.38 0.435 0.49 0 200 400 600 800 1000 1200 1400 1600 1800  Training Validation Testing  Figure 5:   The bar chart for the error   ε a   ≡   log 10   |   f   t  12 | −   log 10   |   f   m  12 | ≡  log 10 ( |   f   t  12 /   f   m  12 | ) for the training (bottom bars), validation (middle bars) and testing (top bars) data. The height of a bar represents the number of samples   N s   in the bin.  The next a few results provide fuller description of the errors. Fig. 4 shows the regression of the training and the testing data, where superscripts   t   and   m   are used to denote the true and modelled values, respectively. Excel- lent regression is obtained for both datasets. The coe ffi - cient of determination   R   is more than 99% in both cases. The bar chart in Fig. 5 shows the distribution of the absolute error for log 10   |   f 12 |   defined by   ε a   ≡   log 10   |   f   t  12 | −  log 10   |   f   m  12 | . Given that the values for log 10   |   f 12 |   approxi- mately range between   − 20 and   − 5 (c.f. Fig. 4), the error on a large majority data points is very small. The error in log 10   |   f 12 |   is small enough that the mag- nitude   |   f 12 |   itself is also accurately modelled. Plotted in Fig. 6 is the relative error for   |   f 12 | , which is defined as  ε r   ≡ ||   f   t  12 |−|   f   m  12 || / |   f   m  12 |   and is related to   ε a   by   ε r   =   | 10 ε a   − 1 | . 5

# Page 6

0   0.2   0.4   0.6   0.8   1   1.2   1.4   1.6   1.8   2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  Training Testing Figure 6: The probability distribution for the relative error   ε r   ≡ | ( |   f   t  12 |− |   f   m  12 | ) | / |   f   m  12 |   for the training (bars) and the testing (filled circles) sets. The   Y -axis is the probability for   ε r   in each bin. The blue line is the cumulative probability for   ε r   for the testing data.  There are a small number of extraneous samples where  ε r   can be more than 100%, but, as expected, the majority of the samples have small errors. The cumulative prob- ability distribution, plotted with the dashed line, shows that more than 90% samples in the testing set have rela- tive errors smaller than 30%, and about 85% smaller than 20%. Comparing the errors on the training set and the testing set, it is only slightly more probable to observe larger errors on the testing data, which demonstrates that the model generalizes well. The above results show that the FFNN model (with 15 neurons on each of the two hidden layers) can provide an accurate model for not only log 10   |   f 12 |   but also   |   f 12 | . In terms of the training cost, typically less than 100 epochs are needed to achieve the accuracy depicted in Figs. 4-6, which takes less than one minute to compute on a modern laptop.  4.2. The support-vector machine model for   sgn   f 12  In its most simple form, the SVM [17] is an algorithm that finds the optimal line that separates two clusters of points on a plane, hence classifying the points into two classes. In order to introduce some necessary basic con- cepts, its formulation is briefly explained. It is assumed that a set of   N   points   x   j   =   ( x   j  1 ,   x   j  2 )   ∈   R 2   (   j   =   1 ,   2 , ...,   N ) is given as the training set. The points are divided into two groups, the positive and negative classes. For sim- plicity, the data are assumed to be separable, i.e., it is assumed that a straight line can be found to separate the two classes, and that the classifications are known and labelled by 1 and   − 1, respectively. The classification of point   x   j   is recorded by   y   j   ∈ {− 1 ,   1 } . The optimal separating line is defined as the line that separates the two groups whilst leaving the largest mar- gins on both sides of the line. Let the equation for the line be   f   ( x )   ≡   w T   x   +   b   =   0, where   w   ∈   R 2   and   b   ∈   R . It can be shown that the best separating line is the solution of the following optimization problem [17]: find   w   and  b   that minimize the objective   || w || 2   such that for all data points ( x   j ,   y   j ),  y   j   f   ( x   j )   ≥   1 .   (7) The optimal objective maximizes the margins on the two sides of the line. The constraints ensure that the points are found on the correct sides of the separating line. The optimal solution is used to classify new data (not in the training set) in the following way. Let ˆ w   and ˆ b   be the optimal solution, and ˆ f   ( x )   ≡   ˆ w T   x   +   ˆ b . The classifica- tion of a data point   x   is given by its label sgn ˆ f   ( x ). In the more general cases where the data are not sepa- rable, two modifications to the above method have been introduced.   Firstly, one may allow a small number of points to be mis-classified, a practice termed using soft margin. Mathematically, this method amounts to replac- ing Eq. 7 by  y   j   f   ( x   j )   ≥   1   −   ξ   j   (8) for   ξ   j   ≥   0. Meanwhile, a penalty term is added to the objective function to limit the magnitude of   ξ   j . For this investigation, the term takes the form of   C   ∑ N j = 1   ξ   j   with  C   being a parameter called the box constraint. A larger   C  introduces larger penalty, hence reduces   ξ   j ’s magnitudes, which means fewer mis-classifications are allowed. No mis-classification is allowed when   C   → ∞ . Secondly, one may use a nonlinear curve to separate the data.   The idea is implemented by using a function  f   ( x )   =   w T   h ( x )   +   b   as the separating curve, where   h ( x ) is a non-linear function that transforms the separating line to a nonlinear separating curve. The SVM model obviously is also applicable for higher dimensional problems. It is used in this investi- gation, but the solution is based on its dual formulation, because the dual problem is convex and guaranteed to converge to the global minimum [7]. Letting   α   j   be the dual variable corresponding to the constraint given in Eq. 8, the dual problem can be written as min   1 2  N ∑  j = 1  N ∑  k = 1  α   j α k y   j y k G ( x   j ,   x k )   −  N ∑  j = 1  α   j   (9) subject to the constraints  N ∑  j = 1  α   j y   j   =   0 ,   0   ≤   α   j   ≤   C .   (10) The bivariate function   G   in Eq. 9 is the kernel function corresponding to the nonlinear function   h ( x ). For this in- vestigation, a Gaussian kernel is used where   G ( x   j ,   x k )   =  exp( −|| x   j   −   x k || 2 /σ 2 ) with   σ   being the kernel scale. 6

# Page 7

The dual problem is solved with the sequential mini- mal optimization (SMO) algorithm. The SMO is an it- erative descent algorithm. The convergence is monitored by the gradient of the objective function (given in Eq. 9) with respect to   α   j , specifically, by the di ff erence between the gradient components corresponding to the maximal upper and lower violation of the feasibility conditions [7, 11]. The iteration is deemed converged when this dif- ference is smaller than a tolerance   δ . Once the optimal solution for the dual problem is found, ˆ w   and ˆ b   can then be found from   α   j   using the Karush-Kuhn-Tucker condi- tions, which are then used to classify a new data point. For more details of the SVM methods and the algorithms, see [17, 7]. 0   20   40   60   80   100 -700 -600 -500 -400 -300 -200 -100 0 0 0.5 1 1.5 2 2.5 3 3.5 4  Figure 7:   The objective function and the gradient di ff erence as func- tions of the training epochs in a typical training session. Only the first 100 epochs are shown.  The SVM model is applied to classify the data for sgn   f 12 , using the MATLAB command fitcsvm which implements the above algorithms.   In this case,   x   is a five dimensional vector, i.e.,   x   =   ( D ,   R E 1 ,   R E 2 ,   p a , ω ) T  and the label   y   is sgn   f 12 .   The number of data points is   N   =   5400. The data are standardized when they are fed into the training algorithm.   Specifically, the mean is removed from the data which are then rescaled by the standard deviation. Fig. 7 illustrates the decay of the gra- dient di ff erence as well as the objective function in Eq. 9 in a typical training process. The adjustable parameters in the model are the box constraint   C , the kernel scale   σ , and the tolerance   δ . Tests with   δ   =   10 − 3   and 10 − 4   show essentially the same results.   Therefore   δ   =   10 − 4   has been used.   When the separation boundary has a complicated shape, a small  σ   is required to model the fine features of the bound- ary.   However a too small   σ   may limit the ability of the model to capture the large scale features in the dis- tribution of the data.   The choice of   C   depends on the accuracy of the data for   f 12 .   If the data for   f 12   likely contain large errors, it is not meaningful to insist perfect classification. The physical model being used here (Eq. 3) has been derived with a few simplifying assumptions. 0   5   10   15   20 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45  Figure 8:   The mis-classification rate for di ff erent kernel scales   σ   and box constraints   C .  Therefore, it is appropriate to use a large   C   to limit mis- classification, although it is not necessary to achieve zero mis-classification. With   the   above   discussion   in   mind,   the   mis- classification rate has been calculated for several dif- ferent kernel scales   σ   and box constraints   C .   A point  x   j   is mis-classified if   y   j   ˆ f   ( x   j )   <   1 (c.f.   Eq.   8).   The mis-classification rate is the ratio of the number of mis- classified points,   N m , to the total number   N . The results are plotted in Fig. 8. The mis-classification rate reaches an approximate plateau quickly when   C   increases. The results for   σ   =   0 . 4 are clearly much worse, whereas small mis-classification is found for other   σ ’s when   C  is su ffi ciently large. These observations demonstrate the robustness of the classification scheme as long as   σ   is not too small. The smallest mis-classification of 2 . 61% is found at ( σ,   C )   =   (0 . 6 ,   20), which is considered su ffi - ciently small. Therefore   C   =   20 and   σ   =   0 . 6 are used in what follows. 1   2   3   4   5   6   7   8   9   10   11 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995 1  Figure 9:   The left axis: the median accuracy (solid line with circles) and the median specificity (dashed line with squares). The right axis: the median precision (solid line with diamonds) and the median recall (dashed line with triangles).   Found in an ensemble of 32 runs.   The error bars show the maximum and minimum.  The robustness of the model is assessed in Fig. 9 using  k -fold cross-validation [17].   In   k -fold cross-validation, 7

# Page 8

the dataset is divided randomly into   k   equal sets, and   k  models (called the partitioned models) are trained. The  i th ( i   =   1 ,   2 , ...,   k ) dataset is called the   i th test fold, whereas the other   k − 1 sets form the   i th training fold. The  i th partitioned SVM model is trained on the   i th training fold and evaluated on the   i th test fold.   The overall as- sessment is based on performance indices averaged over the   k   partitioned models. The most commonly used performance indices are the accuracy, the precision, the recall and the specificity. For each predictor, the model response could either be posi- tive or negative; in either case, it could be either true or false. As a result, the response falls in one of four cate- gories: a positive response could be a true positive (TP) or, coming erroneously from a predictor in the negative class, a false positive (FN), whereas a negative response could be true negative (TN) or false negative (FN). Let  N T P ,   N T N   ,   N FP   and   N FN   be the numbers of TP, TN, FP, and FN, respectively. The accuracy is defined as  N T P   +   N T N  N   ,   (11) which simply gives the percentage of correct predictions in both classes. The precision, recall and specificity are, respectively, defined as  N T P  N T P   +   N FP  ,   N T P  N T P   +   N FN  ,   and   N T N  N T N   +   N FP  .   (12) The precision tells us the probability of a positive re- sponse being correct; the recall is the probability of the positives being correctly identified as positive, while the specificity gives the probability of the negatives being correctly identified as negative [17]. Due to the randomness in data partition, the indices av- eraged over the   k   partitioned models may fluctuate if the cross-validation is conducted multiple times. Therefore, we repeat the validation 32 times to find the medians and ranges of the indices. The results are plotted in Fig. 9. It is clear that the accuracy, the precision, and the recall of the models are consistently high (more than 98% for all of them), although they drop slightly with the number of folds while the ranges increase slightly (the error bars for the accuracy are too narrow to see on the figure). With fewer folds, the number of data points in each fold, hence in the training set, is smaller. Thus the performance of the partitioned models are expected to somewhat deteri- orate. The results for accuracy show that more than 98% data points, with either negative or positive   f 12 , are clas- sified correctly. Meanwhile, the result for the precision shows that there is a 98% chance that   f 12   is indeed pos- itive when the model predicts so, and the result for the recall shows that there is a 1   −   98%   =   2% chance that   f 12  is not predicted to be positive when it is actually positive. Fig. 9 shows that the median specificity and its range display behaviours similar to those of the accuracy, the precision and the recall, but there is stronger dependence on the number of folds, and its range is wider and the median is smaller. The median recall increases with the fold number and reaches approximately 85%, which im- plies there is a 1   −   85%   =   15% chance that   f 12   is not predicted to be negative when it is actually negative. The specificity is relatively low compared with the other in- dices, but this observation does not necessarily reflect poorer performance regarding the samples with negative  f 12 .   Rather, this behaviour is due to the fact that there is only about 3 . 4% data points on which   f 12   <   0, thus mis-classification has an outsized impact. The results presented in this subsection show that, with kernel scale   σ   =   0 . 6, box constraint   C   =   20, and tolerance   δ   =   10 − 4 , the trained SVM classifier can e ff ec- tively model the distribution of the signs of   f 12 .  5. E ffi ciency and accuracy of the combined model 0   5   10   15   20 0 2 4 6 8 10 12 14 16 18 20  Figure 10: The scatter plot for (   f   t  12 / 〈   f   t  12 〉 ,   f   m  12 / 〈   f   m  12 〉 ), where   〈·〉   denotes the averaging over the dataset. 0   0.2   0.4   0.6   0.8   1   1.2   1.4   1.6   1.8   2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  Figure 11:   The probability distribution for the relative error   ε   f r   =  |   f   t  12   −   f   m  12 | / |   f   t  12 | .  8

# Page 9

A prediction for the force factor   f 12   can be made by combining the two ML models obtained in Section 4. In this section the predictions   f   m  12   made this way are com- pared with the true predictions   f   t  12   found from solving Eqs.   3 and 6.   1024 samples for ( D ,   R E 1 ,   R E 2 ,   p a , ω ) T  are randomly chosen, with each component falling in the range covered by the dataset used to train the ML models. Note that the samples are not necessarily in the dataset. Excluding the samples where   R E 1   <   R E 2 , 640 samples are used for this test, and 640 pairs of values (   f   t  12 ,   f   m  12 ) are obtained. Excellent correlation is found between   f   t  12   and   f   m  12 , with the correlation coe ffi cient being 0 . 98.   The scatter plot for the data is shown in Fig. 10. The figure confirms the good correlation while, in the meantime, shows that the di ff erence tends to increase when the magnitude of the force increases. The histogram for the relative error   ε   f r   ≡ |   f   t  12 −   f   m  12 | / |   f   t  12 |  is shown in Fig. 11. The error distribution has a broader spread than those in Fig.   6, i.e., those for the training data and the testing data.   This behaviour is not unex- pected as the ML models are being applied to a new dataset here. Nevertheless, more than 45% samples have less than 20% errors, and for more than 75% samples the error is less than 40%. Therefore, the results are still quite satisfactory. Obviously, the performance of the ML models can be improved if the training data set can be ex- panded and refined. Finally, the ML models are extremely e ffi cient com- pared with direct numerical integration.   It takes about one hour to obtain the 640 values for   f   t  12 , whereas it takes less than one second for the ML models to find corre- sponding   f   m  12 .  6. Conclusions  Machine learning models for the secondary Bjerknes force as a function for several parameters have been de- veloped in a two bubble system. Because the force varies drastically with the parameters, the magnitude and the sign of the force have to be modelled separately, which results in a composite model consisting of a feed-forward neural network for (the logarithm of) the former and a support-vector machine for the latter. Numerical tests demonstrate the feasibility of using machine learning to tackle this problem. Practical meth- ods for choosing the suitable architecture and hyperpa- rameters for the models are proposed. Accurate machine models are obtained, which are shown to be very e ffi cient compared with direct numerical integration of the bubble evolution equations. The results demonstrate that machine learning is a vi- able method in modelling the interactions between bub- bles.   The models developed here have the potential to enhance the future simulations of bubble clusters.   Ob- viously, the model can be further refined and expanded, by, e.g., using a larger dataset covering a wider range of parameters. Machine learning clearly is equally applica- ble when a more sophisticate physical model is used to describe bubble oscillations, although it is not obvious that the currently chosen architectures are still su ffi cient when the dataset grows larger. These interesting topics will be explored in our future investigations.  7. Acknowledgement  The authors gratefully acknowledge the support pro- vided by the Guangzhou Science (Technology) Research Project (Project No. 201704030010) and the special fund project of science and technology innovation strategy of Guangdong Province.  References  [1]   Ahmed, D., Lu, M., Nourhani, A., Lammert, P. E., Stratton, Z., Muddana, H. S., Crespi, V. H., Huang, T. J., 2015. Selec- tively manipulable acoustic-powered microswimmers. Scientific Reports 5, 9744. [2]   Barbat, T., Ashgriz, N., Liu, C.-S., 1999. Dynamics of two inter- acting bubbles in an acoustic field. J. Fluid Mech. 389, 137–168. [3]   Bermudez-Aguirre, D., Mobbs, T., Barbosa-Canovas, G. V., 2011. Ultrasound applications in food processing. In: Feng, H., Barbosa-Cnovas, G. V., Weiss, J. (Eds.), Ultrasound Technolo- gies for food and Bioprocessing. Springer, p. 65. [4]   Brennen, C. E., 1995. Cavitation and bubble dynamics. Oxford University Press. [5]   Brujan,   E.   A.,   2011.   Cavitation   in   Non-Newtonian   Fluids. Springer-Verlag Berlin Heidelberg. [6]   Ciaburro, G., 2017. MATLAB for Machine Learning. Packt Pub- lishing. [7]   Cristianini, N., Shawe-Taylor, J., 2014. An introduction to sup- port vector machines and other kernel-based learning methods. Cambridge University Press. [8]   Crum, L. A., 1975. Bjerknes forces on bubbles in a stationary sound field. The Journal of the Acoustical Society of America 57, 1363. [9]   Doinikov, A. A., Zavtrak, S. T., 1995. On the mutual interaction of two gas bubbles in a sound field. Phys. Fluids 7, 1923. [10]   Eskin, G. I., Eskin, D. G., 2015. Ultrasonic treatment of light alloy metals. CRC Press. [11]   Fan, R. E., Chen, P. H., Lin, C. J., 2005. Working set selection using second order information for training support vector ma- chines. Journal of Machine Learning Research 6, 18891918. [12]   Fan, Z., Chen, D., Deng, C. X., 2014. Characterization of the dy- namic activities of a population of microbubbles driven by pulsed ultrasound exposure in sonoporation. Ultrasound in Medicine and Biology 40, 1260–1272. [13]   Goodfellow, I., Bengio, Y., Courville, A., Bach, F., 2017. Deep Learning. MIT Press. [14]   Hagan, M. T., Demuth, H. B., Beale, M. H., Jesus, O. D., 2014. Neural Network Desgin. Martin Hagan, 2014. [15]   Haghi, H., Sojahrood, A. J., Kolios, M. C., 2019. Collective non- linear behavior of interacting polydisperse microbubble clusters. Ultrasonics - Sonochemistry 58, 104708. [16]   Harkin, A., Kaper, T. J., Nadim, A., 2001. Coupled pulsation and translation of two gas bubbles in a liquid. J. Fluid Mech. 445, 377–411.  9

# Page 10

[17]   Hastie, T., Tibshirani, R., Friedman, J., 2009. The elements of sta- tistical learning: data mining, inference, and prediction. Springer. [18]   Ida, M., 2009. Multibubble cavitation inception. Phys. Fluids 21, 113302. [19]   Jiao, J., He, Y., Kentish, S. E., Ashokkumar, M., Manasseh, R., Lee, J., 2015. Experimental and theoretical analysis of secondary bjerknes forces between two bubbles in a standing wave. Ultra- sonics 58, 35–42. [20]   Keller, J. B., Miksis, M., 1980. Bubble oscillations of large am- plitude. J. Acoust. Soc. Am. 68, 628–633. [21]   Lanoy, M., Derec, C., Tourin, A., Leroy, V., 2015. Manipulating bubbles with secondary bjerknes forces. Appl. Phys. Lett. 107, 214101. [22]   Lazarus, C., Pouliopoulos, A. N., Tinguely, M., Garbin, V., Choi, J. J., 2017. Clustering dynamics of microbubbles exposed to low- pressure 1-mhz ultrasound. J. Acoust. Soc. Am. 142, 3135–3146. [23]   Leighton, T. G., 1994. The Acoustic Bubble. Academic Press, London. [24]   Maeda, K., Colonius, T., 2019. Bubble cloud dynamics in an ul- trasound field. J. Fluid Mech. 862, 1105–1134. [25]   Mettin, R., 2005. Bubble structures in acoustic cavitation. In: Doinikov, A. (Ed.), Bubble and Particle Dynamics in Acoustic Fields: Modern Trends and Applications. Kerala, India: Research Signpost, pp. 1–36. [26]   Mettin, R., Akhatov, I., Parlitz, U., Ohl, C. D., Lauterborn, W., 1997. Bjerknes forces between small cavitation bubbles in a strong acoustic field. Phys. Rev. E 56, 2925. [27]   Mettin, R., Luther, S., Ohl, C.-D., Lauterborn, W., 1999. Acoustic cavitation structures and simulations by a particle model. Ultra- sonics Sonochemistry 6, 25–29. [28]   Pandey, V., 2019. Asymmetricity and sign reversal of secondary bjerknes force from strong nonlinear coupling in cavitation bub- ble pairs. Phys. Rev. E 99, 042209. [29]   Parlitze, U., Mettin, R., Luther, S., Akhatov, I., Voss, M., Lauter- born, W., 1999. Spatio-temporal dynamics of acoustic cavitation bubble clouds. Phil. Trans. R. Soc. Lond. A 357, 313–334. [30]   Pelekasis, N. A., Gaki, A., Doinikov, A., Tsamopoulos, J. A., 2004. Secondary bjerknes forces between two bubbles and the phenomenon of acoustic streamers. J. Fluid Mech. 500, 313–347. [31]   Pelekasis, N. A., Tsamopoulos, J. A., 1993. Bjerknes forces be- tween two bubbles. part 1. response to a step change in pressure. J. Fluid Mech. 254, 467–499. [32]   Pelekasis, N. A., Tsamopoulos, J. A., 1993. Bjerknes forces be- tween two bubbles. part 2. response to an oscillatory pressure field. J. Fluid Mech. 254, 501–527. [33]   Roberts,   W. W.,   2014. Development and translation of his- totripsy: current status and future directions. Curr. Opin. Urol. 24, 104–110. [34]   Silver,   D.,   Schrittwieser,   J.,   Simonyan,   K.,   Antonoglou,   I., Huang, A., Guez, A., adn L. Baker, T. H., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., Hassabis, D., 2017. Mastering the game of Go with- out human knowledge. Nature 550, 354. [35]   Yoshida, K., Fujikawa, T., Watanabe, Y., 2011. Experimental in- vestigation on reversal of secondary bjerknes force between two bubbles in ultrasonic standing wave. The Journal of the Acousti- cal Society of America 130, 135. [36]   Zhang, Y., Zhang, Y., Li, S., 2016. The secondary bjerknes force between two gas bubbles under dual-frequency acoustic excita- tion. Ultrasonics Sonochemistry 29, 129–145.  10

## Figure Captions (auto-extracted)

- **p.4 (Fig 1)** Figure 1: The architecture of the FFNN with two hidden layers having 15 neurons on each. machine learning community rectified linear units (Re- Architecture N,, in each layer LUs) are now recommended for large scale problems, a 1 3
- **p.4** Fig.|1} More details are given in Section [4.1.3] sen to form the training set, while 15% 1s used for vali- dation, and the rest for testing. In each epoch, the current 4.1.2. The training of the FFNN FFNN 1s used to make predictions on the validation data
- **p.5 (Fig 2)** Figure 2: The median and range of the optimal MSE obtained with 20, 15 10 5 0 different FENN models in 32 runs. lo | fi | 810 112
- **p.5 (Fig 3)** Figure 3: The changes of the MSE and the gradient with the epochs °° FP PP LEP EEL PP FPR for a model with the fourth architecture. ® ® ® ® © , > N N N logo (| f12]/1/15])
- **p.6 (Fig 6)** Figure 6: The probability distribution for the relative error &, = |(|f,|- In the more general cases where the data are not sepa- If72DI/1f7%] for the training (bars) and the testing (filled circles) sets. rable, two modifications to the above method have been The Y-axis 1s the probability for Er in each bin. The blue line 1s the introduced. Firstly, one may allow a small number of
- **p.7 (Fig 7)** Figure 7: The objective function and the gradient difference as func- results for 0 = 0.4 are clearly much worse, whereas tions of the training epochs in a typical training session. Only the first small mis-classification is found for other o’s when C 100 epochs are shown. is sufficiently large. These observations demonstrate the
- **p.8 (Fig 9)** Fig. 9]shows that the median specificity and its range Figure 11: The probability distribution for the relative error &/ = display behaviours similar to those of the accuracy, the i> = fial/Ifp,! 8
